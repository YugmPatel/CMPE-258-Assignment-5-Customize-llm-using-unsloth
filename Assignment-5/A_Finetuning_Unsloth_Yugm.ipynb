{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### üìò Text Generation with TinyLlama using Hugging Face Transformers\n",
        "\n",
        "In this section, we demonstrate how to use the **TinyLlama-1.1B-Chat-v1.0** model for **text generation** using the Hugging Face Transformers library.\n",
        "\n",
        "#### ‚úÖ Objective:\n",
        "\n",
        "To generate human-like text completions based on a given prompt using a compact and efficient language model.\n",
        "\n",
        "#### üîß What we‚Äôll do:\n",
        "\n",
        "* Load the **TinyLlama-1.1B-Chat-v1.0** model and tokenizer\n",
        "* Move the model to GPU (if available) for faster inference\n",
        "* Provide a natural language prompt\n",
        "* Use the `.generate()` method to produce a continuation of the prompt\n",
        "\n",
        "#### üì¶ Required Libraries:\n",
        "\n",
        "We use:\n",
        "\n",
        "* `transformers` to load the model and tokenizer\n",
        "* `torch` to run the model on CPU or GPU\n",
        "\n",
        "> ‚ö†Ô∏è Make sure you have a valid **Hugging Face token** to authenticate and download the model from the hub.\n",
        "\n",
        "#### üöÄ Model Overview:\n",
        "\n",
        "**TinyLlama-1.1B-Chat-v1.0** is a small-sized language model optimized for chat and instruction-following tasks. Despite its small size (\\~1.1B parameters), it can produce fluent and coherent responses for many NLP tasks.\n",
        "\n",
        "Let‚Äôs now proceed to load the model and run text generation."
      ],
      "metadata": {
        "id": "mIBAEUTNqzWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "jGUI92LlpzrU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name and Hugging Face token\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "hf_token = \"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\"  # Replace with your own if necessary"
      ],
      "metadata": {
        "id": "zIPdaIEVp15f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model for causal language modeling\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)"
      ],
      "metadata": {
        "id": "JOdo36Hyp5jC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to evaluation mode (optional but recommended)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_lwu6kVp7WD",
        "outputId": "0bc010b2-805a-4c2f-90a4-efa519c00b2a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-21): 22 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_-wFrAWp9Sz",
        "outputId": "e1aa80af-a15e-467e-8e03-7366271fa246"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-21): 22 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input prompt\n",
        "input_text = \"Review: The food was great and service was fast.\\nSentiment:\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)"
      ],
      "metadata": {
        "id": "WaBfdG_rp_uM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id  # Avoid warning for no pad_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "yHB_QCsfqB0p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print output\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Output:\\n\", output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeyQBisbqEIP",
        "outputId": "cd76453a-acb1-46dc-9364-586150ea5b9c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output:\n",
            " Review: The food was great and service was fast.\n",
            "Sentiment: The staff were friendly and helpful.\n",
            "Rating: 5/5. Based on the passage\n"
          ]
        }
      ]
    }
  ]
}